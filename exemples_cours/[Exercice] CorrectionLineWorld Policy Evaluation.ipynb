{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "beryQv5c6FTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Définition d'un line world sous la forme d'un MDP"
      ],
      "metadata": {
        "id": "19mv7NrhVZzx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psNAv57nOCMW"
      },
      "outputs": [],
      "source": [
        "S = [0, 1, 2 ,3, 4]\n",
        "A = [0, 1] # left right\n",
        "R = [-1, 0, 1]\n",
        "T = [0, 4]\n",
        "p = np.zeros((len(S), len(A), len(S), len(R))) # S, A, S_p, R"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in range(len(S)):\n",
        "  for a in range(len(A)):\n",
        "    for s_p in range(len(S)):\n",
        "      for r in range(len(R)):\n",
        "        if s_p == s + 1 and a == 1 and R[r] == 0 and s in [1, 2]:\n",
        "          p[s, a, s_p, r] = 1.0\n",
        "        if s_p == s - 1 and a == 0 and R[r] == 0 and s in [2, 3]:\n",
        "          p[s, a, s_p, r] = 1.0\n",
        "p[3, 1, 4, 2] = 1.0\n",
        "p[1, 0, 0, 0] = 1.0"
      ],
      "metadata": {
        "id": "tGlcGlTA6Tex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice"
      ],
      "metadata": {
        "id": "rnLQGzt4Vzxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implémentez une fonction prenant en paramètre une policy (un tableau numpy)\n",
        "et renvoyant la value de cette policy (un tableau numpy) pour chaque état du line world"
      ],
      "metadata": {
        "id": "s43r9pUuWPtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy : np.ndarray, V: np.ndarray = None) -> np.ndarray:\n",
        "  theta = 0.00001\n",
        "  V = np.random.random(len(S))\n",
        "  for s in T:\n",
        "    V[s] = 0.0\n",
        "  gamma = 0.999\n",
        "  while True:\n",
        "    delta = 0\n",
        "    for s in range(len(S)):\n",
        "      v = V[s]\n",
        "      total = 0\n",
        "      for a in range(len(A)):\n",
        "        pi_s_a = policy[s, a]\n",
        "        for s_p in range(len(S)):\n",
        "          for r in range(len(R)):\n",
        "            total += pi_s_a * p[s, a, s_p, r] * (R[r] + 0.999 * V[s_p])\n",
        "      V[s] = total\n",
        "      delta = np.maximum(delta, np.abs(v - V[s]))\n",
        "    if delta < theta:\n",
        "      return V"
      ],
      "metadata": {
        "id": "_96znS5JWF9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définissez une policy jouant tout le temps à droite.\n",
        "Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ],
      "metadata": {
        "id": "29NHJWr3WTnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pi_right = np.array([[0.0, 1.0] for _ in range(len(S))])\n",
        "print(policy_evaluation(pi_right))"
      ],
      "metadata": {
        "id": "xB8Gd0vPWiue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ead850-6226-494a-816b-ed0ff637e9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.       -0.998001 -0.999    -1.        0.      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définissez une policy jouant tout le temps à gauche. Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ],
      "metadata": {
        "id": "imIKxQ_fW1wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pi_left = np.array([[1.0, 0.0] for _ in range(len(S))])\n",
        "print(policy_evaluation(pi_left))"
      ],
      "metadata": {
        "id": "q-_ZCGmRW31Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eddb5fdf-2017-4855-e711-d3071d1034dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.       1.       0.999    0.998001 0.      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définissez une policy uniformément aléatoire (50% de chance d'aller à gauche et 50% de chances d'aller à droite). Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ],
      "metadata": {
        "id": "N_8nQtoBXJai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pi_random = np.array([[0.5, 0.5] for _ in range(len(S))])\n",
        "print(policy_evaluation(pi_random))"
      ],
      "metadata": {
        "id": "eUYtT2hYW_Fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d1ad5b-ecff-478f-f031-4f4e78760b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.00000000e+00  5.00007008e-01  7.00142750e-06 -4.99996503e-01\n",
            "  0.00000000e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définissez une policy uniformément aléatoire (15% de chance d'aller à gauche et 85% de chances d'aller à droite). Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ],
      "metadata": {
        "id": "XQpwh-x_XXGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pi_weird_random = np.array([[0.15, 0.85] for _ in range(len(S))])\n",
        "print(policy_evaluation(pi_weird_random))"
      ],
      "metadata": {
        "id": "h-HkR7HCXc2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc51aa3e-5959-4b6d-9773-c9cac2875ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.         -0.64651467 -0.93801545 -0.99056161  0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_ineficient_policy_iteration():\n",
        "  theta = 0.000001\n",
        "  gamma = 0.999\n",
        "\n",
        "  V = np.random.random(len(S))\n",
        "  for s in T:\n",
        "    V[s] = 0.0\n",
        "\n",
        "  Pi = np.random.choice(A, len(S))\n",
        "\n",
        "  while True:\n",
        "    # Policy Evaluation\n",
        "    while True:\n",
        "      delta = 0.0\n",
        "\n",
        "      for s in S:\n",
        "        v = V[s]\n",
        "\n",
        "        total = 0.0\n",
        "        for s_p in S:\n",
        "          for r_index in range(len(R)):\n",
        "            total += p[s, Pi[s], s_p, r_index] * (R[r_index] + gamma * V[s_p])\n",
        "\n",
        "        V[s] = total\n",
        "\n",
        "        delta = max(delta, abs(v - V[s]))\n",
        "\n",
        "      if delta < theta:\n",
        "        break\n",
        "\n",
        "    # Policy Improvement\n",
        "    policy_stable = True\n",
        "\n",
        "    for s in S:\n",
        "      old_action = Pi[s]\n",
        "\n",
        "      best_a = None\n",
        "      best_action_score = -9999999\n",
        "\n",
        "      for a in A:\n",
        "        total = 0.0\n",
        "        for s_p in S:\n",
        "          for r_index in range(len(R)):\n",
        "            total += p[s, a, s_p, r_index] * (R[r_index] + gamma * V[s_p])\n",
        "\n",
        "        if best_a is None or total >= best_action_score:\n",
        "          best_a = a\n",
        "          best_action_score = total\n",
        "\n",
        "      Pi[s] = best_a\n",
        "      if Pi[s] != old_action:\n",
        "        policy_stable = False\n",
        "\n",
        "    if policy_stable:\n",
        "      return Pi"
      ],
      "metadata": {
        "id": "nqSUVgCIis1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_ineficient_policy_iteration()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVMOCNl0j3Q-",
        "outputId": "ec4b0b57-c253-40a5-d823-0663e0efbf6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définissez une variante d'un GRID WORLD sous la forme d'un MDP et évaluez différentes stratégies sur cette environnement.\n",
        "\n",
        "Le grid world est une grille de 5x5 cases (5 lignes de 5 colonnes) sur laquelle l'agent peut évoluer, il commence généralement sur la première ligne, première colonne. L'agent possède 4 actions possibles (gauche, droite, haut, bas). Si jamais l'agent atteint la dernière case de la première ligne => état terminal avec reward de -3. Si jamais l'agent atteint la dernière case de la dernière ligne => état terminal avec reward de 1. Si l'agent essaye de se déplacer en dehors des bords de la grille => état terminal avec score de -1."
      ],
      "metadata": {
        "id": "EZshaI9mXiaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S = None #TODO\n",
        "A = None #TODO\n",
        "R = None #TODO\n",
        "p = None #TODO"
      ],
      "metadata": {
        "id": "l_eC0-d9Xh7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proposez plusieurs stratégies et évaluez les à l'aide de policy évaluation."
      ],
      "metadata": {
        "id": "YdZTA1y6YwkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "Ve8dvoONYuq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus : Implémentez policy itération et value itération (dans les slides que nous n'avons pas encore vu) et obtenez à l'aide de ces derniers pi* pour le Line World et le Grid World"
      ],
      "metadata": {
        "id": "ex0zmlgSY4UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "J-vUDJ_0ZJaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Policy iteration implementation in order to find a Pi*"
      ],
      "metadata": {
        "id": "E3fcwX9miANL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_policy_iteration():\n",
        "  theta = 0.00001\n",
        "  gamma = 0.999\n",
        "\n",
        "  V = np.random.random(len(S))\n",
        "  for s in T:\n",
        "    V[s] = 0.0\n",
        "  Pi = np.random.choice(A, len(S), True)\n",
        "\n",
        "  while True:\n",
        "    # Policy evaluation\n",
        "    while True:\n",
        "      delta = 0\n",
        "      for s in range(len(S)):\n",
        "        v = V[s]\n",
        "        total = 0\n",
        "        for s_p in range(len(S)):\n",
        "          for r in range(len(R)):\n",
        "            total += p[s, Pi[s], s_p, r] * (R[r] + 0.999 * V[s_p])\n",
        "        V[s] = total\n",
        "        delta = np.maximum(delta, np.abs(v - V[s]))\n",
        "      if delta < theta:\n",
        "        break\n",
        "\n",
        "    # Policy improvement\n",
        "    policy_stable = True\n",
        "\n",
        "    for s in S:\n",
        "      if s in T:\n",
        "        continue\n",
        "\n",
        "      old_action = Pi[s]\n",
        "\n",
        "      # Compute Arg Max a\n",
        "      argmax_a = None\n",
        "      max_a = -999999999\n",
        "\n",
        "      for a in A:\n",
        "        total = 0.0\n",
        "\n",
        "        for s_p in S:\n",
        "          for r_index in range(len(R)):\n",
        "            total += p[s, a, s_p, r_index] * (R[r_index] + gamma * V[s_p])\n",
        "\n",
        "        if argmax_a is None or total >= max_a:\n",
        "          argmax_a = a\n",
        "          max_a = total\n",
        "\n",
        "      Pi[s] = argmax_a\n",
        "\n",
        "      if old_action != Pi[s]:\n",
        "        policy_stable = False\n",
        "\n",
        "\n",
        "    if policy_stable:\n",
        "      break\n",
        "\n",
        "  return Pi\n"
      ],
      "metadata": {
        "id": "sluiZiiziKRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_policy_iteration()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfiEE-u0jpql",
        "outputId": "806e8de8-49b6-4007-d7af-637fbd359214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N4ONTRgCjr8q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}