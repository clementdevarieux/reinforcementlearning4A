{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "beryQv5c6FTP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Définition d'un line world sous la forme d'un MDP"
      ],
      "metadata": {
        "id": "19mv7NrhVZzx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "psNAv57nOCMW"
      },
      "outputs": [],
      "source": [
        "S = [0, 1, 2 ,3, 4]\n",
        "A = [0, 1] # left right\n",
        "R = [-1, 0, 1]\n",
        "p = np.zeros((len(S), len(A), len(S), len(R))) # S, A, S_p, R"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in range(len(S)):\n",
        "  for a in range(len(A)):\n",
        "    for s_p in range(len(S)):\n",
        "      for r in range(len(R)):\n",
        "        if s_p == s + 1 and a == 1 and R[r] == 0 and s in [1, 2]:\n",
        "          p[s, a, s_p, r] = 1.0\n",
        "        if s_p == s - 1 and a == 0 and R[r] == 0 and s in [2, 3]:\n",
        "          p[s, a, s_p, r] = 1.0\n",
        "p[3, 1, 4, 2] = 1.0\n",
        "p[1, 0, 0, 0] = 1.0"
      ],
      "metadata": {
        "id": "tGlcGlTA6Tex"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice"
      ],
      "metadata": {
        "id": "rnLQGzt4Vzxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implémentez une fonction prenant en paramètre une policy (un tableau numpy)\n",
        "et renvoyant la value de cette policy (un tableau numpy) pour chaque état du line world"
      ],
      "metadata": {
        "id": "s43r9pUuWPtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation_on_line_world(policy : np.ndarray) -> np.ndarray:\n",
        "  #TODO\n",
        "  pass"
      ],
      "metadata": {
        "id": "_96znS5JWF9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définissez une policy jouant tout le temps à droite.\n",
        "Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ],
      "metadata": {
        "id": "29NHJWr3WTnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pi_right = None #TODO\n",
        "print(policy_evaluation_on_line_world(pi_right))"
      ],
      "metadata": {
        "id": "xB8Gd0vPWiue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définissez une policy jouant tout le temps à gauche. Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ],
      "metadata": {
        "id": "imIKxQ_fW1wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pi_left = None #TODO\n",
        "print(policy_evaluation_on_line_world(pi_left))"
      ],
      "metadata": {
        "id": "q-_ZCGmRW31Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définissez une policy uniformément aléatoire (50% de chance d'aller à gauche et 50% de chances d'aller à droite). Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ],
      "metadata": {
        "id": "N_8nQtoBXJai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pi_random = None #TODO\n",
        "print(policy_evaluation_on_line_world(pi_random))"
      ],
      "metadata": {
        "id": "eUYtT2hYW_Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définissez une policy uniformément aléatoire (15% de chance d'aller à gauche et 85% de chances d'aller à droite). Affichez la value de cette policy obtenue grâce à l'algorithme policy evaluation"
      ],
      "metadata": {
        "id": "XQpwh-x_XXGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pi_weird_random = None #TODO\n",
        "print(policy_evaluation_on_line_world(pi_weird_random))"
      ],
      "metadata": {
        "id": "h-HkR7HCXc2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définissez une variante d'un GRID WORLD sous la forme d'un MDP et évaluez différentes stratégies sur cette environnement.\n",
        "\n",
        "Le grid world est une grille de 5x5 cases (5 lignes de 5 colonnes) sur laquelle l'agent peut évoluer, il commence généralement sur la première ligne, première colonne. L'agent possède 4 actions possibles (gauche, droite, haut, bas). Si jamais l'agent atteint la dernière case de la première ligne => état terminal avec reward de -3. Si jamais l'agent atteint la dernière case de la dernière ligne => état terminal avec reward de 1. Si l'agent essaye de se déplacer en dehors des bords de la grille => état terminal avec score de -1."
      ],
      "metadata": {
        "id": "EZshaI9mXiaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S = None #TODO\n",
        "A = None #TODO\n",
        "R = None #TODO\n",
        "p = None #TODO"
      ],
      "metadata": {
        "id": "l_eC0-d9Xh7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proposez plusieurs stratégies et évaluez les à l'aide de policy évaluation."
      ],
      "metadata": {
        "id": "YdZTA1y6YwkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "Ve8dvoONYuq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus : Implémentez policy itération et value itération (dans les slides que nous n'avons pas encore vu) et obtenez à l'aide de ces derniers pi* pour le Line World et le Grid World"
      ],
      "metadata": {
        "id": "ex0zmlgSY4UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "metadata": {
        "id": "J-vUDJ_0ZJaO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}